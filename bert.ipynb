{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "TWk0qYbC2ogk"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import pandas as pd\n",
        "import chardet\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_encoding(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Detects the encoding of a file\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        result = chardet.detect(f.read())\n",
        "\n",
        "    return result['encoding']\n",
        "\n",
        "def preprocess_data(data: pd.DataFrame, target: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Preprocesses the data\n",
        "\n",
        "    Args:\n",
        "        data: The dataframe to preprocess\n",
        "        target: The target to extract from the dataset\n",
        "\n",
        "    Returns:\n",
        "        The preprocessed dataframe\n",
        "    \"\"\"\n",
        "    # Deep copy the dataframe\n",
        "    data = data.copy()\n",
        "\n",
        "    # Remove targets we do not care about\n",
        "    data = data.loc[data['Target'] == target]\n",
        "\n",
        "    # Remove '#SemSt' from the 'Tweet' column\n",
        "    data['Tweet'] = data['Tweet'].str.replace('#SemST', '')\n",
        "\n",
        "    # Make tweets lowercase\n",
        "    data['Tweet'] = data['Tweet'].str.lower()\n",
        "\n",
        "    return data\n",
        "\n",
        "def print_stance_statistics(data: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Prints the number of tweets in each stance\n",
        "    \"\"\"\n",
        "    stance_counts = data['Stance'].value_counts()\n",
        "    print(stance_counts)\n",
        "\n",
        "def load_sem_eval_data(target: str) -> (pd.DataFrame, pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Loads the SemEval 2016 dataset and extracts rows that contain the target\n",
        "\n",
        "    Args:\n",
        "        target: The target (e.g., \"Climate Change is a Real Concern\") to extract from the dataset\n",
        "\n",
        "    Returns:\n",
        "        Two pandas dataframes containing the training and test datasets\n",
        "    \"\"\"\n",
        "\n",
        "    # Load training dataset\n",
        "    train_data_path = '/content/drive/MyDrive/semeval2016-task6-trainingdata.txt'\n",
        "    train_data = pd.read_csv(train_data_path, sep='\\t', encoding=detect_encoding(train_data_path))\n",
        "\n",
        "    # Load test dataset\n",
        "    test_data_path = '/content/drive/MyDrive/SemEval2016-Task6-subtaskA-testdata-gold.txt'\n",
        "    test_data = pd.read_csv(test_data_path, sep='\\t', encoding=detect_encoding(test_data_path))\n",
        "\n",
        "    # Preprocess training and test data\n",
        "    train_data = preprocess_data(train_data, target)\n",
        "    test_data = preprocess_data(test_data, target)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "def split_data(data: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Splits the SemEval-2016 data into training and test data\n",
        "\n",
        "    Args:\n",
        "        data: The dataframe with SemEval-2016 data\n",
        "\n",
        "    Returns:\n",
        "        Two pandas dataframes containing the training and test datasets\n",
        "    \"\"\"\n",
        "    return (data['Tweet'], data['Stance'])\n"
      ],
      "metadata": {
        "id": "iS38MVMV3TNo"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_reddit(data: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Splits the SemEval-2016 data into training and test data\n",
        "\n",
        "    Args:\n",
        "        data: The dataframe with SemEval-2016 data\n",
        "\n",
        "    Returns:\n",
        "        Two pandas dataframes containing the training and test datasets\n",
        "    \"\"\"\n",
        "    return (data['Text'], data['Stance'])"
      ],
      "metadata": {
        "id": "Qqn0H15tywAz"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_score(y_test, y_pred) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the evaluation metric for the SemEval 2016 Task 6, Subtask A\n",
        "    which is the macro-average of the f1-score for \"FAVOR\" and the\n",
        "    f1-score for \"AGAINST\", ignoring the \"NONE\" class.\n",
        "\n",
        "    Args:\n",
        "        y_test: The true labels\n",
        "        y_pred: The predicted labels\n",
        "\n",
        "    Returns:\n",
        "        The calculated F1 score\n",
        "    \"\"\"\n",
        "    mask = (y_test != 'NONE') # Remove \"NONE\" class tweets\n",
        "    y_test_filtered = y_test[mask]\n",
        "    y_pred_filtered = y_pred[mask]\n",
        "\n",
        "    f1 = f1_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "    return f1"
      ],
      "metadata": {
        "id": "1lrUP_ue3QxW"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, weight_decay):\n",
        "    \"\"\"\n",
        "    FROM HOMEWORK 3\n",
        "    Return the optimizer (Adam) you will use to train the model.\n",
        "\n",
        "    Input:\n",
        "        - net: model\n",
        "        - lr: initial learning_rate\n",
        "        - weight_decay: weight_decay in optimizer\n",
        "    \"\"\"\n",
        "    return optim.Adam(params=net.parameters(), lr=lr, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "nKpL2fSm2qPb"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_tensor(labels):\n",
        "    \"\"\"\n",
        "    Convert a list of labels to a tensor\n",
        "\n",
        "    Input:\n",
        "        - labels: list of labels\n",
        "        - label_mapping: dictionary mapping label to index\n",
        "\n",
        "    Output:\n",
        "        - label_tensor: tensor of labels\n",
        "    \"\"\"\n",
        "    label_mapping = {'FAVOR': 2, 'NONE':1, 'AGAINST': 0}\n",
        "\n",
        "    label_nums = labels.map(label_mapping)\n",
        "\n",
        "    return torch.tensor(label_nums.values)"
      ],
      "metadata": {
        "id": "5fhWQTEa2tby"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/gumgum-tech/handling-class-imbalance-by-introducing-sample-weighting-in-the-loss-function-3bdebd8203b4"
      ],
      "metadata": {
        "id": "xy5Lhg8vJiP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_loss_function(weights):\n",
        "    \"\"\"\n",
        "    Return loss fuction. Use class weights to fix lopsided training data\n",
        "    \"\"\"\n",
        "    class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "    return nn.CrossEntropyLoss(weight = class_weights)"
      ],
      "metadata": {
        "id": "Z0lqe8kIG7WQ"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"\n",
        "    Return the device you will use for training/testing.\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VLCGZVvd2une"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_loader(X: pd.DataFrame, y: pd.DataFrame, tokenizer, batch_size: int, max_length=512) -> DataLoader:\n",
        "    \"\"\"\n",
        "    Creates the data loader for SemEval data\n",
        "\n",
        "    Args:\n",
        "        X: The input data, a list of tweets\n",
        "        y: The labels, a list of stances\n",
        "        batch_size: The batch size\n",
        "    \"\"\"\n",
        "    X = X.tolist()\n",
        "    tokenized_inputs = tokenizer(\n",
        "        X,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    data = TensorDataset(\n",
        "        tokenized_inputs[\"input_ids\"],\n",
        "        tokenized_inputs[\"attention_mask\"],\n",
        "        get_label_tensor(y)\n",
        "    )\n",
        "\n",
        "    # X_ids = tokenizer.batch_encode_plus(X,\n",
        "    #                                     padding=True,\n",
        "    #                                     return_tensors='pt')\n",
        "\n",
        "    # y_tensors = get_label_tensor(y)\n",
        "\n",
        "    # data = TensorDataset(X_ids['input_ids'], X_ids['attention_mask'], y_tensors)\n",
        "\n",
        "    return DataLoader(data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "2TG1pDnf2wdR"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'Climate Change is a Real Concern'\n",
        "\n",
        "train_data, test_data = load_sem_eval_data(target)\n",
        "\n"
      ],
      "metadata": {
        "id": "GL19XXbVAahL"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, device, loss_function, num_epochs):\n",
        "    num_itr = 0\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            num_itr += 1\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = loss_function(outputs.logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Epoch No. {0}--Iteration No. {1}-- batch loss = {2:.4f}'.format(\n",
        "            epoch + 1,\n",
        "            num_itr,\n",
        "            loss.item()\n",
        "            ))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MNRgZn462yoH"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into X and y\n",
        "X_train, y_train = split_data(train_data)\n",
        "X_test, y_test = split_data(test_data)\n",
        "\n",
        "# Tokenize to BERT format\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True, padding=\"max_length\")\n",
        "\n",
        "max_length = 512\n",
        "# Create data loaders\n",
        "train_loader = create_loader(X_train, y_train, tokenizer, batch_size=16, max_length=max_length)\n",
        "test_loader = create_loader(X_test, y_test, tokenizer, batch_size=16, max_length=max_length)\n",
        "\n",
        "# Create BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Fine Tune BERT model\n",
        "optimizer = get_optimizer(model, lr=5e-5, weight_decay=0)\n",
        "device = get_device()\n",
        "\n",
        "weights = [5000, 1, 1]\n",
        "loss_function = define_loss_function(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-et6eN9A251I",
        "outputId": "5777558f-21d8-406e-ac82-91a1870f14f7"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = train_model(model, train_loader, optimizer, device, loss_function, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlzoSbcWzOMD",
        "outputId": "1bed6277-2620-4ad9-b524-348f4c731246"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch No. 1--Iteration No. 25-- batch loss = 1.0300\n",
            "Epoch No. 2--Iteration No. 50-- batch loss = 0.9110\n",
            "Epoch No. 3--Iteration No. 75-- batch loss = 0.7470\n",
            "Epoch No. 4--Iteration No. 100-- batch loss = 0.9460\n",
            "Epoch No. 5--Iteration No. 125-- batch loss = 0.9921\n",
            "Epoch No. 6--Iteration No. 150-- batch loss = 0.7886\n",
            "Epoch No. 7--Iteration No. 175-- batch loss = 0.6003\n",
            "Epoch No. 8--Iteration No. 200-- batch loss = 0.0750\n",
            "Epoch No. 9--Iteration No. 225-- batch loss = 0.2361\n",
            "Epoch No. 10--Iteration No. 250-- batch loss = 0.0327\n",
            "Epoch No. 11--Iteration No. 275-- batch loss = 0.0211\n",
            "Epoch No. 12--Iteration No. 300-- batch loss = 0.0134\n",
            "Epoch No. 13--Iteration No. 325-- batch loss = 0.0100\n",
            "Epoch No. 14--Iteration No. 350-- batch loss = 0.0077\n",
            "Epoch No. 15--Iteration No. 375-- batch loss = 0.0084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            # Count the number of correct predictions, ignore the \"NONE\" class\n",
        "            #mask = (labels != 1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_predictions += torch.sum(preds)\n",
        "            preds = preds\n",
        "            labels = labels\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "\n",
        "    # Calculate average F1 score\n",
        "    avg_f1 = sum(f1) / len(f1)\n",
        "\n",
        "    # Print or return the results\n",
        "    print('Precision (favor): {:.4f}'.format(precision[0]))\n",
        "    print('Recall (favor): {:.4f}'.format(recall[0]))\n",
        "    print('F1 Score (favor): {:.4f}'.format(f1[0]))\n",
        "\n",
        "    print('Precision (neutral): {:.4f}'.format(precision[1]))\n",
        "    print('Recall (neutral): {:.4f}'.format(recall[1]))\n",
        "    print('F1 Score (neutral): {:.4f}'.format(f1[1]))\n",
        "\n",
        "    print('Precision (against): {:.4f}'.format(precision[2]))\n",
        "    print('Recall (against): {:.4f}'.format(recall[2]))\n",
        "    print('F1 Score (against): {:.4f}'.format(f1[2]))\n",
        "\n",
        "    print('Average F1 Score: {:.4f}'.format(avg_f1))\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    print(correct_predictions, total_predictions)\n",
        "    accuracy = correct_predictions.double() / total_predictions.double()\n",
        "\n",
        "    print('Test Accuracy: {:.4f}'.format(accuracy))"
      ],
      "metadata": {
        "id": "SnG1Lm_F4av4"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Twitter predictions (Control)')\n",
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsxzvLJ528Yz",
        "outputId": "3f1f4742-eb98-49fb-cce8-ebc7a3c29fec"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision (favor): 1.0000\n",
            "Recall (favor): 0.0909\n",
            "F1 Score (favor): 0.1667\n",
            "Precision (neutral): 0.7353\n",
            "Recall (neutral): 0.7143\n",
            "F1 Score (neutral): 0.7246\n",
            "Precision (against): 0.8582\n",
            "Recall (against): 0.9350\n",
            "F1 Score (against): 0.8949\n",
            "Average F1 Score: 0.5954\n",
            "tensor(141, device='cuda:0') tensor(302, device='cuda:0')\n",
            "Test Accuracy: 0.4669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_reddit_data(tokenizer):\n",
        "    reddit_body_path = '/content/drive/MyDrive/reddit_body_data.txt'\n",
        "    body_test = pd.read_csv(reddit_body_path, sep='\\t', encoding=detect_encoding(reddit_body_path))\n",
        "\n",
        "    # Load test dataset\n",
        "    reddit_title_path = '/content/drive/MyDrive/reddit_title_data.txt'\n",
        "    title_test = pd.read_csv(reddit_title_path, sep='\\t', encoding=detect_encoding(reddit_title_path))\n",
        "\n",
        "    # Preprocess training and test data\n",
        "    body_data = body_test.copy()\n",
        "    title_data = title_test.copy()\n",
        "\n",
        "    body_data['Text'] = body_data['Text'].str.lower()\n",
        "    title_data['Text'] = title_data['Text'].str.lower()\n",
        "    body_data['Text'] = body_data['Text'].fillna('')\n",
        "    title_data['Text'] = title_data['Text'].fillna('')\n",
        "    body_data['Text'] = body_data['Text'].apply(lambda x: ' '.join(tokenizer.tokenize(x)[:512]))\n",
        "    title_data['Text'] = title_data['Text'].apply(lambda x: ' '.join(tokenizer.tokenize(x)[:512]))\n",
        "\n",
        "    label_mapping = {0: 'AGAINST', 1: 'NONE', 2: 'FAVOR'}\n",
        "\n",
        "    # Replace values in the specified column\n",
        "    body_data['Stance'] = body_data['Stance'].replace(label_mapping)\n",
        "    title_data['Stance'] = title_data['Stance'].replace(label_mapping)\n",
        "\n",
        "\n",
        "    return body_data, title_data\n"
      ],
      "metadata": {
        "id": "w7vosHFVwYEu"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_reddit(model, loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            # i think don't do this -> Count the number of correct predictions, ignore the \"NONE\" class\n",
        "            #mask = (labels != 1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_predictions += torch.sum(preds)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(all_preds)\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "\n",
        "    # Calculate average F1 score\n",
        "    avg_f1 = sum(f1) / len(f1)\n",
        "\n",
        "    # Print or return the results\n",
        "    print('Precision (favor): {:.4f}'.format(precision[0]))\n",
        "    print('Recall (favor): {:.4f}'.format(recall[0]))\n",
        "    print('F1 Score (favor): {:.4f}'.format(f1[0]))\n",
        "\n",
        "    print('Precision (neutral): {:.4f}'.format(precision[1]))\n",
        "    print('Recall (neutral): {:.4f}'.format(recall[1]))\n",
        "    print('F1 Score (neutral): {:.4f}'.format(f1[1]))\n",
        "\n",
        "    print('Precision (against): {:.4f}'.format(precision[2]))\n",
        "    print('Recall (against): {:.4f}'.format(recall[2]))\n",
        "    print('F1 Score (against): {:.4f}'.format(f1[2]))\n",
        "\n",
        "    print('Average F1 Score: {:.4f}'.format(avg_f1))\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    print(correct_predictions, total_predictions)\n",
        "    accuracy = correct_predictions.double() / total_predictions.double()\n",
        "\n",
        "    print('Test Accuracy: {:.4f}'.format(accuracy))\n"
      ],
      "metadata": {
        "id": "EogY1qvpu3Y1"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body_data, title_data = load_reddit_data(tokenizer)\n",
        "x_body, y_body = split_reddit(body_data)\n",
        "x_title, y_title = split_reddit(title_data)\n",
        "body_loader = create_loader(x_body, y_body, tokenizer, batch_size=16, max_length=max_length)\n",
        "title_loader = create_loader(x_title, y_title, tokenizer, batch_size=16, max_length=max_length)"
      ],
      "metadata": {
        "id": "YLZSB4TaVrRC"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reddit predictions (Body Only)')\n",
        "evaluate_reddit(model, body_loader, device)"
      ],
      "metadata": {
        "id": "MtsGncaP4B6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0393335b-6d0f-4a21-cec9-e8dcc58f5486"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2]\n",
            "Precision (favor): 0.0000\n",
            "Recall (favor): 0.0000\n",
            "F1 Score (favor): 0.0000\n",
            "Precision (neutral): 0.0588\n",
            "Recall (neutral): 0.0769\n",
            "F1 Score (neutral): 0.0667\n",
            "Precision (against): 0.8481\n",
            "Recall (against): 0.9347\n",
            "F1 Score (against): 0.8893\n",
            "Average F1 Score: 0.3187\n",
            "tensor(230, device='cuda:0') tensor(557, device='cuda:0')\n",
            "Test Accuracy: 0.4129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reddit predictions (Title Only)')\n",
        "evaluate_reddit(model, title_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkI1_S3yyC0X",
        "outputId": "3e148eac-6084-47f9-d25e-1b7a593542d3"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision (favor): 0.1875\n",
            "Recall (favor): 0.2903\n",
            "F1 Score (favor): 0.2278\n",
            "Precision (neutral): 0.0870\n",
            "Recall (neutral): 0.1538\n",
            "F1 Score (neutral): 0.1111\n",
            "Precision (against): 0.8761\n",
            "Recall (against): 0.7796\n",
            "F1 Score (against): 0.8251\n",
            "Average F1 Score: 0.3880\n",
            "tensor(202, device='cuda:0') tensor(459, device='cuda:0')\n",
            "Test Accuracy: 0.4401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "both_data_text = title_data['Text'] + ' ' + body_data['Text']\n",
        "\n",
        "# Create a new DataFrame 'both_data'\n",
        "both_data = pd.DataFrame({'Text': both_data_text, 'Stance': title_data['Stance']})\n",
        "x_both, y_both = split_reddit(both_data)\n",
        "both_loader = create_loader(x_both, y_both, tokenizer, batch_size=16, max_length=max_length)\n",
        "print('Reddit predictions (Title and Body)')\n",
        "evaluate_reddit(model, both_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGYW4JQzV7nY",
        "outputId": "0280652e-4503-4e36-ccec-8cc92ad59003"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision (favor): 0.0000\n",
            "Recall (favor): 0.0000\n",
            "F1 Score (favor): 0.0000\n",
            "Precision (neutral): 1.0000\n",
            "Recall (neutral): 0.0769\n",
            "F1 Score (neutral): 0.1429\n",
            "Precision (against): 0.8497\n",
            "Recall (against): 0.9918\n",
            "F1 Score (against): 0.9153\n",
            "Average F1 Score: 0.3527\n",
            "tensor(244, device='cuda:0') tensor(573, device='cuda:0')\n",
            "Test Accuracy: 0.4258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sT4ZhQp0XI5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}